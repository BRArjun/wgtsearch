{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# COMPLETE GREENTEXT ML DATASET SCRAPER PIPELINE\n",
        "# One-cell solution for Google Colab\n",
        "# Purpose: Scrape r/wholesomegreentext for ML training dataset\n",
        "# ====================================================================\n",
        "\n",
        "# ============== INSTALLATION & SETUP ==============\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Install required packages\n",
        "def install_packages():\n",
        "    packages = ['praw', 'pillow', 'tqdm', 'scikit-learn', 'requests', 'pandas']\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
        "        except:\n",
        "            print(f\"‚ö†Ô∏è Could not install {package}\")\n",
        "    print(\"‚úÖ Package installation complete!\")\n",
        "\n",
        "install_packages()\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_MOUNTED = True\n",
        "    print(\"‚úÖ Google Drive mounted!\")\n",
        "except:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"‚ÑπÔ∏è Google Drive not available\")\n",
        "\n",
        "# ============== IMPORTS ==============\n",
        "import praw\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "from typing import List, Dict, Optional\n",
        "from tqdm.notebook import tqdm\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============== CONFIGURATION ==============\n",
        "# üîß CONFIGURE YOUR CREDENTIALS HERE üîß\n",
        "REDDIT_CONFIG = {\n",
        "    'client_id': 'iJnKy41u_V_xeQ9kz7tkWQ',          # Your Reddit client ID\n",
        "    'client_secret': 'fb0VRa1Nua7blD_AO22oor2aFYPUuA',  # Your Reddit client secret\n",
        "    'user_agent': 'GreentextML/1.0 by BRArjun_890'     # Your user agent\n",
        "}\n",
        "\n",
        "DATASET_CONFIG = {\n",
        "    'subreddit': 'wholesomegreentext',\n",
        "    'hot_posts': 500,           # Number of hot posts to scrape\n",
        "    'top_posts': 500,          # Number of top posts to scrape\n",
        "    'min_score': 50,            # Minimum upvotes for quality\n",
        "    'save_to_drive': DRIVE_MOUNTED,\n",
        "    'create_archive': True\n",
        "}\n",
        "\n",
        "print(\"üîß Configuration loaded!\")\n",
        "\n",
        "# ============== MAIN SCRAPER CLASS ==============\n",
        "class CompleteGreentextPipeline:\n",
        "    def __init__(self, reddit_config: dict, dataset_config: dict):\n",
        "        \"\"\"Initialize the complete pipeline\"\"\"\n",
        "        self.reddit_config = reddit_config\n",
        "        self.dataset_config = dataset_config\n",
        "\n",
        "        # Initialize Reddit connection\n",
        "        self.reddit = praw.Reddit(**reddit_config)\n",
        "\n",
        "        # Test connection\n",
        "        try:\n",
        "            test_sub = self.reddit.subreddit('test')\n",
        "            print(f\"‚úÖ Reddit API connected successfully!\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"‚ùå Reddit API connection failed: {e}\")\n",
        "\n",
        "        # Setup paths\n",
        "        self.setup_directories()\n",
        "\n",
        "    def setup_directories(self):\n",
        "        \"\"\"Setup directory structure\"\"\"\n",
        "        base_name = \"greentext_ml_dataset\"\n",
        "\n",
        "        if self.dataset_config['save_to_drive'] and DRIVE_MOUNTED:\n",
        "            self.base_path = Path(f\"/content/drive/MyDrive/{base_name}\")\n",
        "        else:\n",
        "            self.base_path = Path(f\"/content/{base_name}\")\n",
        "\n",
        "        self.images_path = self.base_path / \"images\"\n",
        "        self.metadata_path = self.base_path / \"metadata\"\n",
        "\n",
        "        # Create directories\n",
        "        self.base_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.images_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.metadata_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        print(f\"üìÅ Dataset directory: {self.base_path}\")\n",
        "\n",
        "    def is_valid_greentext_image(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL is a valid greentext image\"\"\"\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.webp')\n",
        "        valid_domains = ['i.redd.it', 'i.imgur.com', 'imgur.com', 'preview.redd.it']\n",
        "\n",
        "        return (url_lower.endswith(valid_extensions) or\n",
        "                any(domain in url_lower for domain in valid_domains))\n",
        "\n",
        "    def download_image(self, url: str, filename: str, max_retries: int = 3) -> Optional[Dict]:\n",
        "        \"\"\"Download and save image with robust error handling\"\"\"\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, timeout=30)\n",
        "\n",
        "                # Handle different HTTP status codes\n",
        "                if response.status_code == 404:\n",
        "                    print(f\"üîç Image not found (404): {url}\")\n",
        "                    return {'download_success': False, 'error': '404_not_found'}\n",
        "\n",
        "                if response.status_code == 403:\n",
        "                    print(f\"üîí Access forbidden (403): {url}\")\n",
        "                    return {'download_success': False, 'error': '403_forbidden'}\n",
        "\n",
        "                if response.status_code == 429:\n",
        "                    print(f\"‚è≥ Rate limited (429): {url} - Waiting...\")\n",
        "                    time.sleep(60)  # Wait 1 minute for rate limit\n",
        "                    continue\n",
        "\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Verify content is actually an image\n",
        "                content_type = response.headers.get('content-type', '').lower()\n",
        "                if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'png', 'gif', 'webp']):\n",
        "                    print(f\"‚ùå Not an image file: {url} (Content-Type: {content_type})\")\n",
        "                    return {'download_success': False, 'error': 'not_image'}\n",
        "\n",
        "                # Process image\n",
        "                try:\n",
        "                    img = Image.open(io.BytesIO(response.content))\n",
        "                    img.verify()  # Verify image integrity\n",
        "\n",
        "                    # Re-open for processing (verify closes the image)\n",
        "                    img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "                    # Convert problematic modes\n",
        "                    if img.mode in ('RGBA', 'P', 'LA'):\n",
        "                        img = img.convert('RGB')\n",
        "\n",
        "                    # Check image size (avoid tiny or huge images)\n",
        "                    width, height = img.size\n",
        "                    if width < 50 or height < 50:\n",
        "                        print(f\"üìè Image too small: {url} ({width}x{height})\")\n",
        "                        return {'download_success': False, 'error': 'too_small'}\n",
        "\n",
        "                    if width > 5000 or height > 5000:\n",
        "                        print(f\"üìè Image too large: {url} ({width}x{height}) - Resizing...\")\n",
        "                        img.thumbnail((2000, 2000), Image.Resampling.LANCZOS)\n",
        "\n",
        "                    # Save image\n",
        "                    image_path = self.images_path / filename\n",
        "                    img.save(image_path, 'JPEG', quality=95, optimize=True)\n",
        "\n",
        "                    return {\n",
        "                        'filename': filename,\n",
        "                        'size_bytes': len(response.content),\n",
        "                        'dimensions': img.size,\n",
        "                        'download_success': True,\n",
        "                        'error': None\n",
        "                    }\n",
        "\n",
        "                except (OSError, IOError) as img_error:\n",
        "                    print(f\"üñºÔ∏è Image processing failed: {url} - {img_error}\")\n",
        "                    return {'download_success': False, 'error': 'image_processing_failed'}\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"‚è∞ Timeout (attempt {attempt + 1}/{max_retries}): {url}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                    continue\n",
        "                return {'download_success': False, 'error': 'timeout'}\n",
        "\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                print(f\"üåê Connection error (attempt {attempt + 1}/{max_retries}): {url}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "                    continue\n",
        "                return {'download_success': False, 'error': 'connection_error'}\n",
        "\n",
        "            except requests.exceptions.RequestException as req_error:\n",
        "                print(f\"üì° Request failed: {url} - {req_error}\")\n",
        "                return {'download_success': False, 'error': f'request_failed: {req_error}'}\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Unexpected error downloading {url}: {e}\")\n",
        "                return {'download_success': False, 'error': f'unexpected: {e}'}\n",
        "\n",
        "        return {'download_success': False, 'error': 'max_retries_exceeded'}\n",
        "\n",
        "    def extract_post_features(self, submission) -> Dict:\n",
        "        \"\"\"Extract ML features from Reddit post\"\"\"\n",
        "        filename = f\"{submission.id}.jpg\"\n",
        "        created_time = datetime.fromtimestamp(submission.created_utc)\n",
        "\n",
        "        return {\n",
        "            'post_id': submission.id,\n",
        "            'filename': filename,\n",
        "            'title': submission.title,\n",
        "            'url': submission.url,\n",
        "            'permalink': f\"https://www.reddit.com{submission.permalink}\",\n",
        "            'score': submission.score,\n",
        "            'upvote_ratio': submission.upvote_ratio,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'title_word_count': len(submission.title.split()),\n",
        "            'title_char_count': len(submission.title),\n",
        "            'domain': submission.domain,\n",
        "            'author': str(submission.author) if submission.author else '[deleted]',\n",
        "            'created_utc': submission.created_utc,\n",
        "            'created_date': created_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'is_nsfw': submission.over_18,\n",
        "            'flair': submission.link_flair_text if submission.link_flair_text else '',\n",
        "            'quality_score': submission.score * submission.upvote_ratio,\n",
        "            'engagement_ratio': submission.num_comments / max(submission.score, 1)\n",
        "        }\n",
        "\n",
        "    def get_subreddit_stats(self, subreddit_name: str) -> Dict:\n",
        "        \"\"\"Get comprehensive subreddit statistics before scraping\"\"\"\n",
        "        try:\n",
        "            print(f\"üìä Analyzing r/{subreddit_name}...\")\n",
        "            subreddit = self.reddit.subreddit(subreddit_name)\n",
        "\n",
        "            # Basic subreddit info\n",
        "            stats = {\n",
        "                'name': subreddit.display_name,\n",
        "                'title': subreddit.title,\n",
        "                'subscribers': subreddit.subscribers,\n",
        "                'description': subreddit.public_description[:200] + \"...\" if len(subreddit.public_description) > 200 else subreddit.public_description,\n",
        "                'created_utc': subreddit.created_utc,\n",
        "                'over18': subreddit.over18\n",
        "            }\n",
        "\n",
        "            # Count posts in different categories\n",
        "            print(\"üîç Counting posts in different categories...\")\n",
        "\n",
        "            # Count hot posts\n",
        "            hot_count = 0\n",
        "            try:\n",
        "                for _ in subreddit.hot(limit=1000):\n",
        "                    hot_count += 1\n",
        "                    if hot_count >= 1000:\n",
        "                        break\n",
        "            except:\n",
        "                hot_count = \"Unable to count\"\n",
        "\n",
        "            # Count top posts (this month)\n",
        "            top_month_count = 0\n",
        "            try:\n",
        "                for _ in subreddit.top(time_filter='month', limit=1000):\n",
        "                    top_month_count += 1\n",
        "                    if top_month_count >= 1000:\n",
        "                        break\n",
        "            except:\n",
        "                top_month_count = \"Unable to count\"\n",
        "\n",
        "            # Count top posts (all time) - sample first 1000\n",
        "            top_all_count = 0\n",
        "            try:\n",
        "                for _ in subreddit.top(time_filter='all', limit=1000):\n",
        "                    top_all_count += 1\n",
        "                    if top_all_count >= 1000:\n",
        "                        break\n",
        "            except:\n",
        "                top_all_count = \"Unable to count\"\n",
        "\n",
        "            # Count new posts\n",
        "            new_count = 0\n",
        "            try:\n",
        "                for _ in subreddit.new(limit=1000):\n",
        "                    new_count += 1\n",
        "                    if new_count >= 1000:\n",
        "                        break\n",
        "            except:\n",
        "                new_count = \"Unable to count\"\n",
        "\n",
        "            stats.update({\n",
        "                'hot_posts_sample': hot_count,\n",
        "                'top_month_posts_sample': top_month_count,\n",
        "                'top_all_posts_sample': top_all_count,\n",
        "                'new_posts_sample': new_count\n",
        "            })\n",
        "\n",
        "            return stats\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error getting subreddit stats: {e}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def display_subreddit_info(self, stats: Dict):\n",
        "        \"\"\"Display subreddit information in a user-friendly format\"\"\"\n",
        "        if 'error' in stats:\n",
        "            print(f\"‚ùå Could not get subreddit information: {stats['error']}\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä SUBREDDIT ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üè∑Ô∏è  Name: r/{stats['name']}\")\n",
        "        print(f\"üìù Title: {stats['title']}\")\n",
        "        print(f\"üë• Subscribers: {stats['subscribers']:,}\")\n",
        "        print(f\"üìÖ Created: {datetime.fromtimestamp(stats['created_utc']).strftime('%Y-%m-%d')}\")\n",
        "        print(f\"üîû NSFW: {'Yes' if stats['over18'] else 'No'}\")\n",
        "        print(f\"üìÑ Description: {stats['description']}\")\n",
        "\n",
        "        print(\"\\nüìà POST COUNTS (Sample of up to 1000 each):\")\n",
        "        print(f\"üî• Hot posts: {stats['hot_posts_sample']}\")\n",
        "        print(f\"‚≠ê Top (this month): {stats['top_month_posts_sample']}\")\n",
        "        print(f\"üèÜ Top (all time): {stats['top_all_posts_sample']}\")\n",
        "        print(f\"üÜï New posts: {stats['new_posts_sample']}\")\n",
        "\n",
        "        print(\"\\nüí° SCRAPING PLAN:\")\n",
        "        print(f\"üì• Hot posts to scrape: {self.dataset_config['hot_posts']}\")\n",
        "        print(f\"üì• Top posts to scrape: {self.dataset_config['top_posts']}\")\n",
        "        print(f\"üìä Minimum score filter: {self.dataset_config['min_score']}\")\n",
        "        print(f\"üéØ Expected total: ~{self.dataset_config['hot_posts'] + self.dataset_config['top_posts']} posts\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_user_confirmation(self) -> bool:\n",
        "        \"\"\"Get user confirmation before starting scraping\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚ö†Ô∏è  BEFORE WE START:\")\n",
        "        print(\"‚Ä¢ This will download images and may take 15-45 minutes\")\n",
        "        print(\"‚Ä¢ Large datasets will use significant storage space\")\n",
        "        print(\"‚Ä¢ Reddit API has rate limits - scraping may be slow\")\n",
        "        print(\"‚Ä¢ Some images may fail to download (404, etc.)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                choice = input(\"\\nü§î Do you want to continue with scraping? (y/n): \").lower().strip()\n",
        "                if choice in ['y', 'yes']:\n",
        "                    print(\"‚úÖ Starting scraping process...\")\n",
        "                    return True\n",
        "                elif choice in ['n', 'no']:\n",
        "                    print(\"‚ùå Scraping cancelled by user.\")\n",
        "                    return False\n",
        "                else:\n",
        "                    print(\"Please enter 'y' for yes or 'n' for no.\")\n",
        "            except (EOFError, KeyboardInterrupt):\n",
        "                print(\"\\n‚ùå Operation cancelled.\")\n",
        "                return False\n",
        "\n",
        "    def scrape_posts(self, sort_method: str, limit: int) -> List[Dict]:\n",
        "        \"\"\"Scrape posts from subreddit\"\"\"\n",
        "        subreddit_name = self.dataset_config['subreddit']\n",
        "        min_score = self.dataset_config['min_score']\n",
        "\n",
        "        print(f\"üîÑ Scraping {limit} {sort_method} posts from r/{subreddit_name}\")\n",
        "\n",
        "        subreddit = self.reddit.subreddit(subreddit_name)\n",
        "\n",
        "        # Get submissions\n",
        "        if sort_method == 'hot':\n",
        "            submissions = subreddit.hot(limit=limit * 2)  # Get extra to account for filtering\n",
        "        elif sort_method == 'top':\n",
        "            submissions = subreddit.top(limit=limit * 2, time_filter='all')\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported sort method: {sort_method}\")\n",
        "\n",
        "        dataset = []\n",
        "        downloaded = 0\n",
        "\n",
        "        pbar = tqdm(desc=f\"Processing {sort_method}\", unit=\"posts\")\n",
        "\n",
        "        for submission in submissions:\n",
        "            pbar.update(1)\n",
        "\n",
        "            # Quality filters\n",
        "            if submission.score < min_score:\n",
        "                continue\n",
        "\n",
        "            if not self.is_valid_greentext_image(submission.url):\n",
        "                continue\n",
        "\n",
        "            # Stop if we have enough\n",
        "            if len(dataset) >= limit:\n",
        "                break\n",
        "\n",
        "            # Extract features\n",
        "            post_data = self.extract_post_features(submission)\n",
        "\n",
        "            # Download image\n",
        "            image_metadata = self.download_image(submission.url, post_data['filename'])\n",
        "\n",
        "            if image_metadata:\n",
        "                post_data.update(image_metadata)\n",
        "                dataset.append(post_data)\n",
        "                downloaded += 1\n",
        "                pbar.set_postfix({'downloaded': downloaded, 'score': submission.score})\n",
        "\n",
        "            time.sleep(0.1)  # Be respectful\n",
        "\n",
        "        pbar.close()\n",
        "        print(f\"‚úÖ Downloaded {downloaded} {sort_method} posts\")\n",
        "        return dataset\n",
        "\n",
        "    def save_dataset(self, df: pd.DataFrame, name: str):\n",
        "        \"\"\"Save dataset with metadata\"\"\"\n",
        "        # Save CSV\n",
        "        csv_path = self.metadata_path / f\"{name}.csv\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # Save info\n",
        "        info = {\n",
        "            'created_at': datetime.now().isoformat(),\n",
        "            'total_posts': len(df),\n",
        "            'images_downloaded': len(df[df['download_success'] == True]),\n",
        "            'score_range': [df['score'].min(), df['score'].max()],\n",
        "            'date_range': [df['created_date'].min(), df['created_date'].max()],\n",
        "            'average_quality': df['quality_score'].mean(),\n",
        "            'subreddit': self.dataset_config['subreddit']\n",
        "        }\n",
        "\n",
        "        info_path = self.metadata_path / f\"{name}_info.json\"\n",
        "        with open(info_path, 'w') as f:\n",
        "            json.dump(info, f, indent=2)\n",
        "\n",
        "        print(f\"üíæ Saved: {csv_path}\")\n",
        "        return csv_path\n",
        "\n",
        "    def create_train_test_split(self, df: pd.DataFrame):\n",
        "        \"\"\"Create ML train/test split\"\"\"\n",
        "        # Stratify by score quintiles for balanced split\n",
        "        score_bins = pd.cut(df['score'], bins=5, labels=False)\n",
        "\n",
        "        train_df, test_df = train_test_split(\n",
        "            df, test_size=0.2, random_state=42, stratify=score_bins\n",
        "        )\n",
        "\n",
        "        # Save splits\n",
        "        train_path = self.metadata_path / \"train_set.csv\"\n",
        "        test_path = self.metadata_path / \"test_set.csv\"\n",
        "\n",
        "        train_df.to_csv(train_path, index=False)\n",
        "        test_df.to_csv(test_path, index=False)\n",
        "\n",
        "        print(f\"üîÑ Train/Test split: {len(train_df)} / {len(test_df)}\")\n",
        "        return train_df, test_df\n",
        "\n",
        "    def create_archive(self):\n",
        "        \"\"\"Create downloadable zip archive\"\"\"\n",
        "        if not self.dataset_config['create_archive']:\n",
        "            return None\n",
        "\n",
        "        archive_path = self.base_path.parent / \"greentext_ml_dataset.zip\"\n",
        "\n",
        "        print(\"üì¶ Creating archive...\")\n",
        "        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file_path in self.base_path.rglob('*'):\n",
        "                if file_path.is_file():\n",
        "                    arcname = file_path.relative_to(self.base_path)\n",
        "                    zipf.write(file_path, arcname)\n",
        "\n",
        "        size_mb = os.path.getsize(archive_path) / (1024 * 1024)\n",
        "        print(f\"üì¶ Archive created: {archive_path} ({size_mb:.1f} MB)\")\n",
        "        return archive_path\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete ML dataset creation pipeline with user confirmation\"\"\"\n",
        "        print(\"üöÄ GREENTEXT ML DATASET SCRAPER\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Step 1: Get subreddit statistics\n",
        "        stats = self.get_subreddit_stats(self.dataset_config['subreddit'])\n",
        "\n",
        "        # Step 2: Display information and get user confirmation\n",
        "        if not self.display_subreddit_info(stats):\n",
        "            print(\"‚ùå Unable to get subreddit information. Exiting.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        if not self.get_user_confirmation():\n",
        "            print(\"üëã Goodbye! Run the script again when you're ready.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Step 3: Start the actual scraping process\n",
        "        print(\"\\nüî• STARTING DATASET CREATION PIPELINE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Phase 1: Scrape hot posts\n",
        "        hot_posts = self.scrape_posts('hot', self.dataset_config['hot_posts'])\n",
        "\n",
        "        # Phase 2: Scrape top posts\n",
        "        top_posts = self.scrape_posts('top', self.dataset_config['top_posts'])\n",
        "\n",
        "        # Check if we got any data\n",
        "        if not hot_posts and not top_posts:\n",
        "            print(\"‚ùå No posts were successfully scraped. Please check your configuration and try again.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Phase 3: Combine and deduplicate\n",
        "        print(\"üîÑ Combining and deduplicating datasets...\")\n",
        "        all_posts = hot_posts + top_posts\n",
        "\n",
        "        if not all_posts:\n",
        "            print(\"‚ùå No posts to process. Exiting.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        df = pd.DataFrame(all_posts)\n",
        "\n",
        "        # Remove duplicates and posts with failed downloads\n",
        "        original_count = len(df)\n",
        "        df = df[df['download_success'] == True]  # Only keep successful downloads\n",
        "        df = df.drop_duplicates(subset=['post_id']).reset_index(drop=True)\n",
        "\n",
        "        print(f\"üìä Dataset processing results:\")\n",
        "        print(f\"   üì• Total posts attempted: {original_count}\")\n",
        "        print(f\"   ‚úÖ Successfully downloaded: {len(df)}\")\n",
        "        print(f\"   üîÑ After deduplication: {len(df)}\")\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(\"‚ùå No valid posts remaining after filtering. Exiting.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Phase 4: Save complete dataset\n",
        "        try:\n",
        "            self.save_dataset(df, \"greentext_complete\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saving dataset: {e}\")\n",
        "            return df, None, None, None\n",
        "\n",
        "        # Phase 5: Create train/test split\n",
        "        try:\n",
        "            if len(df) < 10:\n",
        "                print(\"‚ö†Ô∏è Dataset too small for train/test split. Skipping split creation.\")\n",
        "                train_df, test_df = df, pd.DataFrame()\n",
        "            else:\n",
        "                train_df, test_df = self.create_train_test_split(df)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error creating train/test split: {e}\")\n",
        "            train_df, test_df = df, pd.DataFrame()\n",
        "\n",
        "        # Phase 6: Create archive\n",
        "        try:\n",
        "            archive_path = self.create_archive()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error creating archive: {e}\")\n",
        "            archive_path = None\n",
        "\n",
        "        # Phase 7: Generate final summary\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        print(\"\\nüéâ PIPELINE COMPLETED!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes\")\n",
        "        print(f\"üìä Final dataset size: {len(df)} posts\")\n",
        "        print(f\"üñºÔ∏è  Images downloaded: {len(df[df['download_success']])}\")\n",
        "\n",
        "        if len(train_df) > 0:\n",
        "            print(f\"üèãÔ∏è  Training samples: {len(train_df)}\")\n",
        "        if len(test_df) > 0:\n",
        "            print(f\"üß™ Test samples: {len(test_df)}\")\n",
        "\n",
        "        print(f\"üíæ Dataset location: {self.base_path}\")\n",
        "\n",
        "        if archive_path:\n",
        "            print(f\"üì¶ Download archive: {archive_path}\")\n",
        "\n",
        "        # Quality metrics\n",
        "        if len(df) > 0:\n",
        "            avg_score = df['score'].mean()\n",
        "            avg_quality = df['quality_score'].mean()\n",
        "            print(f\"\\nüìà Quality metrics:\")\n",
        "            print(f\"   ‚≠ê Average post score: {avg_score:.1f}\")\n",
        "            print(f\"   üéØ Average quality score: {avg_quality:.1f}\")\n",
        "\n",
        "        print(\"\\n‚ú® Your ML dataset is ready for training!\")\n",
        "        print(\"üöÄ Next steps: Use the CSV files for ML model training\")\n",
        "\n",
        "        return df, train_df, test_df, archive_path\n",
        "\n",
        "# ============== RUN PIPELINE ==============\n",
        "def main():\n",
        "    \"\"\"Main execution function with comprehensive error handling\"\"\"\n",
        "    try:\n",
        "        print(\"üî• GREENTEXT ML DATASET SCRAPER\")\n",
        "        print(\"Initializing pipeline...\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Create pipeline with error handling\n",
        "        try:\n",
        "            pipeline = CompleteGreentextPipeline(REDDIT_CONFIG, DATASET_CONFIG)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to initialize pipeline: {e}\")\n",
        "            print(\"üí° Check your Reddit API credentials and try again.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Run complete pipeline\n",
        "        result = pipeline.run_complete_pipeline()\n",
        "\n",
        "        if result == (None, None, None, None):\n",
        "            print(\"‚ùå Pipeline execution cancelled or failed.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        dataset, train_set, test_set, archive = result\n",
        "\n",
        "        # Display sample data if available\n",
        "        if dataset is not None and len(dataset) > 0:\n",
        "            print(\"\\nüìã DATASET PREVIEW:\")\n",
        "            print(dataset[['post_id', 'title', 'score', 'quality_score', 'download_success']].head())\n",
        "\n",
        "        return pipeline, dataset, train_set, test_set, archive\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n‚ö†Ô∏è Pipeline interrupted by user (Ctrl+C)\")\n",
        "        return None, None, None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Pipeline failed with unexpected error: {e}\")\n",
        "        print(\"üí° Please check your configuration and try again.\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# ============== EXECUTE ==============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üî• GREENTEXT ML DATASET SCRAPER\")\n",
        "    print(\"Starting automated pipeline...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    pipeline, dataset, train_set, test_set, archive = main()\n",
        "\n",
        "    print(\"\\nüéØ NEXT STEPS:\")\n",
        "    print(\"1. Check your dataset files in the created directory\")\n",
        "    print(\"2. Download the archive if needed\")\n",
        "    print(\"3. Use the train/test CSV files for ML training\")\n",
        "    print(\"4. Images are saved as JPEGs in the images/ folder\")\n",
        "    print(\"\\nüöÄ Happy machine learning!\")"
      ],
      "metadata": {
        "id": "EqaDK_bVGh4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}